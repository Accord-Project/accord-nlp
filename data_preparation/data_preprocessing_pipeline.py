# -*- coding: utf-8 -*-
"""Data_Preprocessing_Pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nuKBBE3zDhpKqvHKprg4j5s00N3adJqB

# Removal of unnecessary phrases
"""

#created by Amna Dridi on 07 Feb 2023
import re

removal_list = ["Approved Document","online version","O N L I N E   V E R S I O N","o n l i n e   v e r s i o n","ONLINE VERSIONONLINE VERSION","for use in england"]
chapter = 'Text-Approved_Doc_G-Content.txt'
try:
    with open(chapter, 'r') as fr:
        lines = fr.readlines()
        min_len = 2
        #print(lines)
        with open('Cleaned-'+chapter, 'w') as fw:
            for line in lines:
                if len(line.strip('\n')) > min_len:
                    new_line = re.sub(r"[^a-zA-Z0-9\.,;:/)('-' ]", "", line)
                    #new_line = new_line.lower()
                    for word in removal_list:
                      new_line = new_line.replace(word, "")
                      #print(new_line)
                    fw.write(new_line)
                    fw.write('\n')
                    fw.write('\n')

    print("Text is cleaned!")
except:
    print("Oops! something error")

with open('Cleaned-'+chapter, 'r') as f:
  nblines = len(f.readlines())
  print('Total Number of lines:', nblines)

"""# Sentence Segementation

"""

import nltk
nltk.download('punkt')
chapter = 'Text-Approved_Doc_G-Content.txt'
with open('Cleaned-'+chapter) as fr:
 contents = fr.read()
corpus = nltk.sent_tokenize(contents)
#print(corpus)
with open('Sentences-'+chapter, 'w') as fsentences:
  for i in range(len(corpus )):
     fsentences.write(corpus[i].strip('\n'))
     fsentences.write('\n')

print ("All sentences have been extracted")
print ('number of sentences in this document: ', len(corpus ))

"""# Sentences Filtering

"""

import nltk
nltk.download('punkt')

keywords = ["greater than", "equal", "less than", "should be", "must", "shall", "could", "prohibit", "at least", "higher than", "lower than", "more than", "This Decree", "means", "refers to", "exceed"]
countFilteredSentences = 0
chapter = 'Sentences-Text-Approved_Doc_G-Content.txt'
with open(chapter) as fr:
 contents = fr.read()
corpus = nltk.sent_tokenize(contents)
with open('Filtered-'+chapter, 'w') as fFilteredSentences:
  for i in range(len(corpus )):
    for word in keywords:
      if word in corpus[i]:
        countFilteredSentences+=1
        fFilteredSentences.write(corpus[i]+'\n')
        fFilteredSentences.write('\n')
        fFilteredSentences.write('\n')
        break

print ("All sentences have been filtered")
print ('number of sentences in this document: ', len(corpus ))
print ('number of filtered sentences in this document: ', countFilteredSentences)


# After this step, the sentences are manually filtered and saved into files with prefix 'Manually'.

"""#Removal of new lines"""

import nltk
import re
nltk.download('punkt')

countFilteredSentences = 0
chapter = 'Filtered-Sentences-Text-Approved_Doc_G-Content.txt'
with open('Manually'+chapter) as fr:
 contents = fr.read()
corpus = nltk.sent_tokenize(contents)
#print(corpus)
with open('All-Semi-'+chapter, 'w') as fFilteredSentences:
  for i in range(len(corpus )):
        countFilteredSentences+=1
        fFilteredSentences.write(re.sub('\n', ' ', corpus[i]))
       # fFilteredSentences.write(corpus[i]+'\n')
        fFilteredSentences.write('\n')

print ("All sentences have been created!")
print ('number of sentences in this document: ', len(corpus ))
print ('number of filtered sentences in this document: ', countFilteredSentences)

"""# Save sentences in csv file

"""

import csv
chapter = 'All-Semi-Filtered-Sentences-Text-Approved_Doc_G-Content.txt'
domain = 'DocG_Sanitation'
with open(chapter) as file_, open('CSV-'+chapter+'.csv', 'w') as csvfile:
    lines = [x for x in file_.read().strip().split('\n') if x]
    writer = csv.writer(csvfile)
    writer.writerow(('ID', 'Text'))
    for idx, line in enumerate(lines, 1):
        writer.writerow((str(idx)+"_UK_"+domain, line.strip('\n')))